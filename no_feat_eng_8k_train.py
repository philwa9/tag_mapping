# -*- coding: utf-8 -*-
"""no_feat_eng_8k_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XkWzf0Fao7ijo3Nx7yuiyHUvsl_85dQU
"""

import pandas as pd
import numpy as np
import re
!pip install transformers


file_path = '/content/mapped_7k_clean.xlsx'
sheet_name = '8k_train'
df = pd.read_excel(file_path, sheet_name=sheet_name)



from transformers.models.roberta_prelayernorm.modeling_roberta_prelayernorm import RobertaPreLayerNormEncoder
if 'gt öfter vorhanden?' in df.columns:
    df = df.drop('gt öfter vorhanden?', axis=1)


# strings
df['gt'] = df['gt'].astype(str)
df['lt'] = df['lt'].astype(str)

df['desc_gt'] = df['desc_gt'].astype(str)
df['desc_lt'] = df['desc_lt'].astype(str)

df['u_gt'] = df['u_gt'].astype(str)
df['u_lt'] = df['u_lt'].astype(str)


# pattern (side_id und alles nach komma weg)
pattern_sid = r'[A-Z]{2}\d{6}'

pattern_descgt = r',[^,]*$'

pattern_machine = r':[A-Za-z]{3,7}\d{1,2}'

pattern_ = r'_[A-Z]{0,3}'


#%%

machines_from_master = [
    'BAC',
    'CAB',
    'CBoost',
    'CGen',
    'CO2C',
    'COC',
    'CTurb',
    'CWP',
    'FeedC',
    'FGB',
    'GANC',
    'Gen',
    'GOXC',
    'H2C',
    'LPBoost',
    'LPGen',
    'LPTurb',
    'MAC',
    'NH3C',
    'PAIRC',
    'RecC',
    'Ref',
    'RefC',
    'SynC',
    'VP',
    'WBoost',
    'WGen',
    'WTurb',
    'LiqTurb',
    'SteamTurb',
    'GasTurb',
    'CTF',
    'LOXP',
    'LINP',
    'LARP',
    'EJF',
    'CCM',
    'IPC',
    'CWS',
    'DCAC',
    'DCWC',
    'FeedPT',
    'GSFR',
    'MHE',
    'PPU',
    'PreRef',
    'PSA',
    'Reform',
    'RHE',
    'Shift',
    'SWGR',
    'Tank',
    'Tank',
    'TRF',
    'Stripper',
    'Liquifier',
    'PreCl',
    'Chlr',
    'Cond',
    'Rcv',
    'Econ',
    'SubCl',
    'Dryer',
    'Catox',
    'LPCol',
    'MPCol',
    'HPCol',
    'CArCol',
    'PArCol',
    'PAirC'
    'KrXeCol',
    'SpargeTank',
    'LOXTank',
    'LOXVAP',
    'LINTANK',
    'LARTank',
    'LINVAP',
    'RECC',
    'LOXTANK',
    'GOX',
    'ASC',
    'GAN',
]


##### Versuch, alle Machines mit dem muster zu erreichen. Problem: mit einer Zahl dahinter erkennt er sie, mit zwei nicht####

machine_tag = []
for index, entry in enumerate(df['gt']):
    match = None
    for machine in machines_from_master:
        pattern = rf'\b{machine}\d'
        if re.search(pattern, entry):
            match = re.findall(pattern, entry)
            break
    if match:
        machine_tag.append(match[0])
        df.at[index, 'gt'] = df.at[index, 'gt'].replace(match[0], '')


#%%
# Create a DataFrame from the machine_tag list
df_machine_tag = pd.DataFrame(machine_tag, columns=['Machine Tag'])

# Replace the ':' character in the 'Machine Tag' column # der hier löscht wieder nur nach dem standard pattern ohne machine_master
df_machine_tag['Machine Tag'] = df_machine_tag['Machine Tag'].str.replace(':', '')

df_machine_tag['Machine Tag'] = df_machine_tag['Machine Tag'].dropna()
df_machine_tag.dropna(subset=['Machine Tag'], inplace=True)


# aus 6 spalten 2 machen

df['gt'] = df.apply(lambda row: ' '.join([str(row['gt'])]), axis=1)

df['lt'] = df.apply(lambda row: ' '.join([str(row['lt']), str(row['desc_lt']), str(row['u_lt'])]), axis=1)


# restliche spalten löschen

df = df.drop(['desc_gt', 'u_gt', 'desc_lt', 'u_lt'], axis=1)

#%%
df['gt'] = df['gt'].str.replace(pattern_sid, '')

df['gt'] = df['gt'].str.replace(':', '')

# notgedrungen scheiß funktion die die erste digit löscht:

            # unbedingt fixen!!

def remove_first_digit(entry):
    if entry[0].isdigit():
        return entry[1:]
    return entry

df['gt'] = df['gt'].apply(remove_first_digit)

unique_entries = df['gt'].unique()

import tensorflow as tf
from transformers import TFBertModel, BertTokenizer
from transformers import RobertaTokenizer, TFRobertaModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


# Tokenize and encode your text data using BERT tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

max_length = 50  # Maximum sequence length for BERT
X_encoded = []

for text in df['lt']:
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='np'
    )
    X_encoded.append(encoding)

import numpy as np


# Tokenize and encode your text data using BERT tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
max_length = 50  # Set your desired max sequence length here

X_encoded = []
for text in df['lt']:
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='np'
    )
    X_encoded.append(encoding)

# Convert the list of encoded inputs to numpy arrays
X_input = np.concatenate([encoding['input_ids'] for encoding in X_encoded])
X_attention = np.concatenate([encoding['attention_mask'] for encoding in X_encoded])

# label encode gt
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['gt'])

# Split the data into training and testing sets (which you already did manually)
X_train, X_test, y_train, y_test = train_test_split(X_input, y, test_size=0.1, random_state=42)

# Split the attention masks into training and testing sets (which should match the previous split)
X_attention_train, X_attention_test, _, _ = train_test_split(X_attention, y, test_size=0.1, random_state=42)

# Build your model with BERT embeddings
input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32)
attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32)

bert_model = TFRobertaModel.from_pretrained('roberta-base')
bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]
output = tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')(bert_output[:, 0, :])

model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)

# Compile and train the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit([X_train, X_attention_train], y_train,
                    epochs=30, batch_size=128,
                    validation_data=([X_test, X_attention_test], y_test))

"""Hier werden Losses und Accuracy geplottet

"""

import matplotlib.pyplot as plt

# Retrieve the training and validation metrics
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# Plot the train and validation losses
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Train and Validation Loss')
plt.legend()

# Plot the train and validation accuracies
plt.subplot(1, 2, 2)
plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Train and Validation Accuracy')
plt.legend()

# Display the chart
plt.tight_layout()
plt.show()

"""Classification Report

"""

import numpy as np
from sklearn.metrics import classification_report

print(X_test.size)
print(X_attention_test.size)

#%%

# Perform predictions on the test set
y_pred = np.argmax(model.predict([X_test, X_attention_test]), axis=-1)

#%%

# Inverse transform the label encodings
y_test_labels = label_encoder.inverse_transform(y_test)
y_pred_labels = label_encoder.inverse_transform(y_pred)

#%%

# Calculate precision, recall, and F1 score
report = classification_report(y_test_labels, y_pred_labels)
print("Classification Report:\n", report)

"""Confusion Matrix"""

import matplotlib.pyplot as plt
import seaborn as sns
import random
from sklearn.metrics import confusion_matrix


#### Random Samples aus Daten für die CM ######
# Generate confusion matrix
cm = confusion_matrix(y_test_labels, y_pred_labels)
print("Confusion Matrix:\n", cm)

# Retrieve original labels from label encoder
original_labels = label_encoder.inverse_transform(range(len(label_encoder.classes_)))

# Randomly select  entries
random_indices = random.sample(range(len(cm)), 25)
random_cm = cm[random_indices, :][:, random_indices]
random_labels = original_labels[random_indices]

# Create a figure and axis
plt.figure(figsize=(10, 10), dpi=150)  # Adjust figsize and dpi as needed

sns.heatmap(random_cm, annot=True, fmt="d", cmap="Blues", cbar=False, xticklabels=random_labels, yticklabels=random_labels)

# Set font size of y-axis tick labels
font_size_axis = 8
plt.xticks(fontsize=font_size_axis)
plt.yticks(fontsize=font_size_axis)

# Set font size of annotations
font_size_param = 8
plt.rcParams['font.size'] = font_size_param

# Set axis labels and title
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Random Confusion Matrix")

# Improve the layout
plt.tight_layout()

# Display the plot
plt.show()

"""Excel für Test laden"""

# Load the data
file_path = '/content/mapped_7k_clean.xlsx'
sheet_name = '8k_test'
test_df = pd.read_excel(file_path, sheet_name=sheet_name)

if 'gt öfter vorhanden?' in test_df.columns:
    test_df = test_df.drop('gt öfter vorhanden?', axis=1)

"""Test input so processen wie bei training"""

# strings
test_df['gt'] = test_df['gt'].astype(str)
test_df['lt'] = test_df['lt'].astype(str)

test_df['desc_gt'] = test_df['desc_gt'].astype(str)
test_df['desc_lt'] = test_df['desc_lt'].astype(str)

test_df['u_gt'] = test_df['u_gt'].astype(str)
test_df['u_lt'] = test_df['u_lt'].astype(str)


# pattern (side_id und alles nach komma weg)
pattern_sid = r'[A-Z]{2}\d{6}'

pattern_descgt = r',[^,]*$'

pattern_machine = r':[A-Za-z]{3,7}\d{1,2}'

pattern_ = r'_[A-Z]{0,3}'


#%%

machines_from_master = [
    'BAC',
    'CAB',
    'CBoost',
    'CGen',
    'CO2C',
    'COC',
    'CTurb',
    'CWP',
    'FeedC',
    'FGB',
    'GANC',
    'Gen',
    'GOXC',
    'H2C',
    'LPBoost',
    'LPGen',
    'LPTurb',
    'MAC',
    'NH3C',
    'PAIRC',
    'RecC',
    'Ref',
    'RefC',
    'SynC',
    'VP',
    'WBoost',
    'WGen',
    'WTurb',
    'LiqTurb',
    'SteamTurb',
    'GasTurb',
    'CTF',
    'LOXP',
    'LINP',
    'LARP',
    'EJF',
    'CCM',
    'IPC',
    'CWS',
    'DCAC',
    'DCWC',
    'FeedPT',
    'GSFR',
    'MHE',
    'PPU',
    'PreRef',
    'PSA',
    'Reform',
    'RHE',
    'Shift',
    'SWGR',
    'Tank',
    'Tank',
    'TRF',
    'Stripper',
    'Liquifier',
    'PreCl',
    'Chlr',
    'Cond',
    'Rcv',
    'Econ',
    'SubCl',
    'Dryer',
    'Catox',
    'LPCol',
    'MPCol',
    'HPCol',
    'CArCol',
    'PArCol',
    'PAirC'
    'KrXeCol',
    'SpargeTank',
    'LOXTank',
    'LOXVAP',
    'LINTANK',
    'LARTank',
    'LINVAP',
    'RECC',
    'LOXTANK',
    'GOX',
    'ASC',
    'GAN',
]


##### Versuch, alle Machines mit dem muster zu erreichen. Problem: mit einer Zahl dahinter erkennt er sie, mit zwei nicht####

machine_tag = []
for index, entry in enumerate(test_df['gt']):
    match = None
    for machine in machines_from_master:
        pattern = rf'\b{machine}\d'
        if re.search(pattern, entry):
            match = re.findall(pattern, entry)
            break
    if match:
        machine_tag.append(match[0])
        test_df.at[index, 'gt'] = test_df.at[index, 'gt'].replace(match[0], '')


#%%
# Create a DataFrame from the machine_tag list
test_df_machine_tag = pd.DataFrame(machine_tag, columns=['Machine Tag'])

# Replace the ':' character in the 'Machine Tag' column # der hier löscht wieder nur nach dem standard pattern ohne machine_master
test_df_machine_tag['Machine Tag'] = test_df_machine_tag['Machine Tag'].str.replace(':', '')

test_df_machine_tag['Machine Tag'] = test_df_machine_tag['Machine Tag'].dropna()
test_df_machine_tag.dropna(subset=['Machine Tag'], inplace=True)


# aus 6 spalten 2 machen

test_df['gt'] = test_df.apply(lambda row: ' '.join([str(row['gt'])]), axis=1)

test_df['lt'] = test_df.apply(lambda row: ' '.join([str(row['lt']), str(row['desc_lt']), str(row['u_lt'])]), axis=1)


# restliche spalten löschen

test_df = test_df.drop(['desc_gt', 'u_gt', 'desc_lt', 'u_lt'], axis=1)

#%%
test_df['gt'] = test_df['gt'].str.replace(pattern_sid, '')

test_df['gt'] = test_df['gt'].str.replace(':', '')

# notgedrungen scheiß funktion die die erste digit löscht:

            # unbedingt fixen!!

def remove_first_digit(entry):
    if entry[0].isdigit():
        return entry[1:]
    return entry

test_df['gt'] = test_df['gt'].apply(remove_first_digit)

unique_entries = test_df['gt'].unique()

"""Funktion für Predictions

"""

def predict_with_confidence(model, tokenizer, lt_input):
    # Tokenize and encode the new input using BERT tokenizer
    encoding = tokenizer.encode_plus(
        lt_input,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='np'
    )

    # Get the input IDs and attention mask
    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']

    # Make the prediction using the trained model
    prediction = model.predict([input_ids, attention_mask])

    # Get the predicted class index
    predicted_class_index = np.argmax(prediction)

    # Get the confidence/probability of the predicted class
    confidence = prediction[0, predicted_class_index]

    # Decode the predicted class using the label encoder
    predicted_class = label_encoder.classes_[predicted_class_index]

    return predicted_class, confidence

# Beispielaufruf der Funktion mit einem neuen Eingangstext
lt_input_example = "Das ist ein Beispieltext für die Vorhersage."
predicted_class, confidence = predict_with_confidence(model, tokenizer, lt_input_example)

# Ausgabe der Vorhersage und Confidence
print("Predicted Class:", predicted_class)
print("Confidence:", confidence)

"""Iteriere durch test_df"""

# Annahme: Du hast einen Testdatenrahmen namens df_test mit den Spalten 'gt' und 'lt' (echtes Label und Input)
# Erstelle leere Spalten für die Vorhersage und die Konfidenz
test_df['prediction'] = ''
test_df['confidence'] = 0.0  # Initialisiere die Konfidenz mit 0.0

# Iteriere über die Zeilen des Testdatenrahmens
for index, row in test_df.iterrows():
    lt_input = row['lt']  # Nehme den Input aus der 'lt'-Spalte

    # Mache die Vorhersage mit der Funktion
    predicted_class, confidence = predict_with_confidence(model, tokenizer, lt_input)

    # Schreibe die Vorhersage und die Konfidenz in den Datenrahmen
    test_df.at[index, 'prediction'] = predicted_class
    test_df.at[index, 'confidence'] = confidence

# Annahme: df_test wurde bereits erstellt und aktualisiert

# Speichere den Testdatenrahmen in eine Excel-Datei
excel_filename = "/content/evaluation_test.xlsx"
test_df.to_excel(excel_filename, index=False)

# Gib eine Bestätigung aus
print(f"Der Testdatenrahmen wurde in die Excel-Datei {excel_filename} gespeichert.")

!pip install fuzzywuzzy


from fuzzywuzzy import fuzz

# Funktion zur Berechnung der Fuzzy-Ähnlichkeit
def calculate_fuzzy_score(row):
    gt = row['gt']
    pred_gt = row['prediction']

    # Verwende die Funktion "fuzz.token_sort_ratio" für die Fuzzy-Ähnlichkeitsberechnung
    score = fuzz.token_sort_ratio(gt, pred_gt)

    return score

# Füge die "fuzzy_score"-Spalte dem DataFrame hinzu
test_df['fuzzy_score'] = test_df.apply(calculate_fuzzy_score, axis=1)

# Speichere den Testdatenrahmen in eine Excel-Datei
excel_filename = "/content/evaluation_test.xlsx"
test_df.to_excel(excel_filename, index=False)

# Gib eine Bestätigung aus
print(f"Der Testdatenrahmen wurde in die Excel-Datei {excel_filename} gespeichert.")

