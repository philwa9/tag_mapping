# -*- coding: utf-8 -*-
"""Train on 7k_BERT_tagspec_v2_new_feature_engineering_25/07/23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fCMXAccisAdSsoDSIUfxg62QH-dEQbDB
"""

import pandas as pd
import numpy as np
import re
!pip install transformers


file_path = '/content/mapped_7k_clean.xlsx'
sheet_name = '7k_list'
df = pd.read_excel(file_path, sheet_name=sheet_name)

##################

# Änderungen 22.06.23:
    # der machine_master wurde jetzt benutzt zum filtern und löschen
    # es gibt eine Funktion, mit der man die Einträge aussortieren kann, wo die anzahl der inputs unter X ist

# Änderungen 23.06.23:
    # lt werden preprocessed (lower() und alle zeichen weg)
    # bein inferieren (top5-Funktion) werden die lt_inputs so preprocessed, dass sie in der gleichen Form wie beim Training vorliegen
# Änderungen 24.06.23:
    # RoBERTa als Tokenizer und Model definiert

# Alle globalen Variablen löschen
globals().clear()

df.to_excel('/content/sample_data/raw_tags.xlsx', index=False)

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from transformers.models.roberta_prelayernorm.modeling_roberta_prelayernorm import RobertaPreLayerNormEncoder
if 'gt öfter vorhanden?' in df.columns:
    df = df.drop('gt öfter vorhanden?', axis=1)


# strings
df['gt'] = df['gt'].astype(str)
df['lt'] = df['lt'].astype(str)

df['desc_gt'] = df['desc_gt'].astype(str)
df['desc_lt'] = df['desc_lt'].astype(str)

df['u_gt'] = df['u_gt'].astype(str)
df['u_lt'] = df['u_lt'].astype(str)

#%%


#%%


# pattern (side_id und alles nach komma weg)
pattern_sid = r'[A-Z]{2}\d{6}'

pattern_descgt = r',[^,]*$'

pattern_machine = r':[A-Za-z]{3,7}\d{1,2}'

pattern_ = r'_[A-Z]{0,3}'


replace_list = ['sdfsdfcsdf']

#%%

machines_from_master = [
    'BAC',
    'CAB',
    'CBoost',
    'CGen',
    'CO2C',
    'COC',
    'CTurb',
    'CWP',
    'FeedC',
    'FGB',
    'GANC',
    'Gen',
    'GOXC',
    'H2C',
    'LPBoost',
    'LPGen',
    'LPTurb',
    'MAC',
    'NH3C',
    'PAIRC',
    'RecC',
    'Ref',
    'RefC',
    'SynC',
    'VP',
    'WBoost',
    'WGen',
    'WTurb',
    'LiqTurb',
    'SteamTurb',
    'GasTurb',
    'CTF',
    'LOXP',
    'LINP',
    'LARP',
    'EJF',
    'CCM',
    'IPC',
    'CWS',
    'DCAC',
    'DCWC',
    'FeedPT',
    'GSFR',
    'MHE',
    'PPU',
    'PreRef',
    'PSA',
    'Reform',
    'RHE',
    'Shift',
    'SWGR',
    'Tank',
    'Tank',
    'TRF',
    'Stripper',
    'Liquifier',
    'PreCl',
    'Chlr',
    'Cond',
    'Rcv',
    'Econ',
    'SubCl',
    'Dryer',
    'Catox',
    'LPCol',
    'MPCol',
    'HPCol',
    'CArCol',
    'PArCol',
    'PAirC'
    'KrXeCol',
    'SpargeTank',
    'LOXTank',
    'LOXVAP',
    'LINTANK',
    'LARTank',
    'LINVAP',
    'RECC',
    'LOXTANK',
    'GOX',
    'ASC',
    'GAN',
]


##### Versuch, alle Machines mit dem muster zu erreichen. Problem: mit einer Zahl dahinter erkennt er sie, mit zwei nicht####

machine_tag = []
for index, entry in enumerate(df['gt']):
    match = None
    for machine in machines_from_master:
        pattern = rf'\b{machine}\d'
        if re.search(pattern, entry):
            match = re.findall(pattern, entry)
            break
    if match:
        machine_tag.append(match[0])
        df.at[index, 'gt'] = df.at[index, 'gt'].replace(match[0], '')



#%%
# Create a DataFrame from the machine_tag list
df_machine_tag = pd.DataFrame(machine_tag, columns=['Machine Tag'])

# Replace the ':' character in the 'Machine Tag' column # der hier löscht wieder nur nach dem standard pattern ohne machine_master
df_machine_tag['Machine Tag'] = df_machine_tag['Machine Tag'].str.replace(':', '')

df_machine_tag['Machine Tag'] = df_machine_tag['Machine Tag'].dropna()
df_machine_tag.dropna(subset=['Machine Tag'], inplace=True)

#%%

# side id und loc weg
df['gt'] = df['gt'].str.replace(pattern_sid, '')
df['lt'] = df['lt'].str.replace(pattern_sid, '')

df['desc_gt'] = df['desc_gt'].str.replace(pattern_descgt, '')

# df['gt'] = df['gt'].str.replace(pattern_machine, '')
df['gt'] = df['gt'].str.replace(pattern_, '')

df['gt'] = df['gt'].str.replace(':', '')
df['lt'] = df['lt'].str.replace(':', '')

# 1st 2nd etc entfernen

number_dict = {'1st': '1', '1rst':'1', '2nd':'2', '3rd':'3', '4th':'4', '5th': '5', '6th': '6', '7th':'7', '8th':'8','9th':'9','10th':'10', '1ST':'1','2ND':'2','3RD':'3', '4TH':'4', '5TH': '5', '6TH': '6', '7TH':'7', '8TH':'8', '9TH':'9', '10TH':'10' }


pattern = r'\b(' + '|'.join(number_dict.keys()) + r')\b'
df['desc_lt'] = df['desc_lt'].str.replace(pattern, lambda x: number_dict[x.group()], regex=True)
df['desc_lt'] = df['desc_lt'].fillna(' ')


# die ersten ein bis zwei buchstaben vom lt weg und neue spalte aufmachen, dann lt spalte modifizieren

df['lt_modified'] = df['lt'].str.extract(r'^([A-Z]{1,2})')
df['lt'] = df['lt'].str.replace(r'^([A-Z]{1,2})', '')

df['lt_modified'] = df['lt_modified'].fillna(' ')

dict_lt_mod = {'T': 'temperature', 'PC':'pressure', 'PI':'pressure', 'P':'pressure', 'F':'flow', 'SC':'speed', 'FC': 'flow', 'G': 'vibration', 'L':'level', 'LI':'level','LC':'level','PI':'pressure','TI':'temperature', 'TC':'temperature', 'HS': 'ONOFF', 'XI': 'vibration x', 'ZI':'vibration z', 'X':'vibration x', 'EA':'ONOFF', 'FI':'flow', 'VY':'vibration y', 'VX':'vibration x', 'VZ':'vibration z', 'M':'motor', 'TR':'temperature', 'GI': 'vibration', 'SI':'speed', 'E':'electric', 'EI':'electric' }

df['lt_modified'] = df['lt_modified'].map(dict_lt_mod).fillna('')
df['process_var'] = df['lt'].str.split('_', n=1).str[1]
df['lt'] = df['lt'].str.split('_', n=1).str[0]


df['lt_modified'] = df['lt_modified'].fillna('')
df['process_var'] = df['process_var'].fillna('')
df['lt'] = df['lt'] + ' ' + df['lt_modified'] + ' ' + df['process_var']

# alles nach komma und location löschen / wörter zu löschen löschen

# pattern_desclt = r',[^,]*$'

# df['desc_lt'] = df['desc_lt'].str.replace(pattern_desclt, ' ')

words_to_delete = ['Hamburg - Finkenwerder', 'Duisburg', 'Scunthorpe', 'IJmuiden', 'Ijmuiden', 'Salaise', 'Hamburg', 'Herne', 'ASU', 'Salzgitter', 'Salzgitter.ASU05', 'Worms', 'Stolberg', 'Marl', 'Bremen', 'Buna', 'Eisenhuettenstadt', 'Gablingen', 'Herne', 'Rothenbach', 'Wetzlar']

df['desc_lt'] = df['desc_lt'].replace(words_to_delete, '', regex=True)

# umlaute ersetzen

df['desc_lt'] = df['desc_lt'].replace(['ä', 'ö', 'ü', 'Ä', 'Ö', 'Ü'], ['ae', 'oe', 'ue', 'Ae', 'Oe', 'Ue'], regex=True)

# rd st first etc hinter zahlen lö

# Iterate over each row in df
for index, row in df.iterrows():
    desc_gt = row['desc_gt']

    # Split the sentence into words
    words = desc_gt.split()

    # Iterate over each word in the sentence
    for word in words:
        if word in df_machine_tag['Machine Tag'].values:
            # Remove the word from the sentence
            desc_gt = desc_gt.replace(word, '')

    # Update the 'desc_gt' column with the modified sentence
    df.at[index, 'desc_gt'] = desc_gt
#%%

# aus 6 spalten 2 machen

df['gt'] = df.apply(lambda row: ' '.join([str(row['gt'])]), axis=1)

df['lt'] = df.apply(lambda row: ' '.join([str(row['lt']), str(row['desc_lt']), str(row['u_lt'])]), axis=1)


# restliche spalten löschen

df = df.drop(['desc_gt', 'u_gt', 'desc_lt', 'u_lt', 'lt_modified', 'process_var' ], axis=1)


##### clean lt #####
##### clean lt #####
##### clean lt #####
##### was wird #####
import unicodedata
import re

# remove_diacritics function
def remove_diacritics(input_string):
    nfkd_form = unicodedata.normalize('NFKD', input_string)
    return ''.join([c for c in nfkd_form if not unicodedata.combining(c)])

df['lt'] = df['lt'].apply(remove_diacritics)

# apply lowercase
df['lt'] = df['lt'].str.lower()

# remove special characters
df['lt'] = df['lt'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', ' ', x))

##### clean lt #####
##### clean lt #####
##### clean lt #####
#%%
# notgedrungen scheiß funktion die die erste digit löscht:
# notgedrungen scheiß funktion die die erste digit löscht:

            # unbedingt fixen!!

def remove_first_digit(entry):
    if entry[0].isdigit():
        return entry[1:]
    return entry

df['gt'] = df['gt'].apply(remove_first_digit)



#%%
unique_entries = df['gt'].unique()

#%%
# Create a dictionary to store unique sentences from column A and their corresponding sentences from column B
sentence_mapping = {}

# Iterate over the rows of the DataFrame
for index, row in df.iterrows():
    gt = row['gt']
    lt = row['lt']

    # Check if the sentence from column A already exists in the dictionary
    if gt in sentence_mapping:
        # If it exists, append the corresponding sentence from column B
        sentence_mapping[gt].append(lt)
    else:
        # If it doesn't exist, create a new key with the sentence from column A
        sentence_mapping[gt] = [lt]


#%%

unique_machine = df_machine_tag['Machine Tag'].unique()

df.to_excel('/content/sample_data/processed_tags2.xlsx', index=False)

#%%


################################################################################################
######################## Diese section filtert die Einträge mit nur x zugeordneten Inputs ########################
##################################################################################################################
threshold = 0

# Filter out entries with three or fewer inputs per label
filtered_mapping = {}

for label, entry in sentence_mapping.items():
    if isinstance(entry, list) and len(entry) > threshold:
        inputs = entry[3]  # Assuming the inputs are stored at index 3
        if len(inputs) > threshold:
            filtered_mapping[label] = entry
    else:
        print(f"Incomplete entry for label '{label}'")

# Print the filtered dictionary
# for label, entry in filtered_mapping.items():
    # print(label, entry)


#%%

import pandas as pd

# Assuming your sentence_mapping dictionary is stored in `sentence_mapping`

# Create a list to store the filtered entries
filtered_entries = []

# Iterate over the sentence_mapping dictionary
for label, inputs in filtered_mapping.items():
    for input_text in inputs:
        filtered_entries.append([label, input_text])

# Create the filtered DataFrame
df = pd.DataFrame(filtered_entries, columns=['gt', 'lt'])

unique_machine = df_machine_tag['Machine Tag'].unique()
unique_entries = df['gt'].unique()


################################################################################################
######################## Diese section filtert die Einträge mit nur x zugeordneten Inputs ########################
##################################################################################################################

#%%

import tensorflow as tf
from transformers import TFBertModel, BertTokenizer
from transformers import RobertaTokenizer, TFRobertaModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


# Tokenize and encode your text data using BERT tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

max_length = 50  # Maximum sequence length for BERT
X_encoded = []

for text in df['lt']:
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='np'
    )
    X_encoded.append(encoding)

import numpy as np


# Tokenize and encode your text data using BERT tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
max_length = 50  # Set your desired max sequence length here

X_encoded = []
for text in df['lt']:
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='np'
    )
    X_encoded.append(encoding)

# Convert the list of encoded inputs to numpy arrays
X_input = np.concatenate([encoding['input_ids'] for encoding in X_encoded])
X_attention = np.concatenate([encoding['attention_mask'] for encoding in X_encoded])

# label encode gt
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['gt'])

# Split the data into training and testing sets (which you already did manually)
X_train, X_test, y_train, y_test = train_test_split(X_input, y, test_size=0.1, random_state=42)

# Split the attention masks into training and testing sets (which should match the previous split)
X_attention_train, X_attention_test, _, _ = train_test_split(X_attention, y, test_size=0.1, random_state=42)

# Build your model with BERT embeddings
input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32)
attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32)

bert_model = TFRobertaModel.from_pretrained('roberta-base')
bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]
output = tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')(bert_output[:, 0, :])

model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)

# Compile and train the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit([X_train, X_attention_train], y_train,
                    epochs=47, batch_size=128,
                    validation_data=([X_test, X_attention_test], y_test))

"""Hier werden die Losses und Accuracies geplottet"""

import matplotlib.pyplot as plt

# Retrieve the training and validation metrics
train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

# Plot the train and validation losses
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Train and Validation Loss')
plt.legend()

# Plot the train and validation accuracies
plt.subplot(1, 2, 2)
plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Train and Validation Accuracy')
plt.legend()

# Display the chart
plt.tight_layout()
plt.show()

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

print(X_test.size)
print(X_attention_test.size)

#%%

# Perform predictions on the test set
y_pred = np.argmax(model.predict([X_test, X_attention_test]), axis=-1)

#%%

# Inverse transform the label encodings
y_test_labels = label_encoder.inverse_transform(y_test)
y_pred_labels = label_encoder.inverse_transform(y_pred)

#%%

# Calculate precision, recall, and F1 score
report = classification_report(y_test_labels, y_pred_labels)
print("Classification Report:\n", report)

import matplotlib.pyplot as plt
import seaborn as sns

# Generate confusion matrix
cm = confusion_matrix(y_test_labels, y_pred_labels)
print("Confusion Matrix:\n", cm)

# Retrieve original labels from label encoder
original_labels = label_encoder.inverse_transform(range(len(label_encoder.classes_)))

# Create a figure and axis
plt.figure(figsize=(20, 20), dpi=150)  # Adjust figsize and dpi as needed

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False, xticklabels=original_labels, yticklabels=original_labels)

# Set font size of y-axis tick labels
font_size_axis = 4
plt.xticks(fontsize=font_size_axis)
plt.yticks(fontsize=font_size_axis)


# Set font size of annotations
font_size_param = 4
plt.rcParams['font.size'] = font_size_param

# Set axis labels and title
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")

# Improve the layout
plt.tight_layout()
# Display the plot
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import random
from sklearn.metrics import confusion_matrix


#### Random Samples aus Daten für die CM ######
# Generate confusion matrix
cm = confusion_matrix(y_test_labels, y_pred_labels)
print("Confusion Matrix:\n", cm)

# Retrieve original labels from label encoder
original_labels = label_encoder.inverse_transform(range(len(label_encoder.classes_)))

# Randomly select  entries
random_indices = random.sample(range(len(cm)), 25)
random_cm = cm[random_indices, :][:, random_indices]
random_labels = original_labels[random_indices]

# Create a figure and axis
plt.figure(figsize=(10, 10), dpi=150)  # Adjust figsize and dpi as needed

sns.heatmap(random_cm, annot=True, fmt="d", cmap="Blues", cbar=False, xticklabels=random_labels, yticklabels=random_labels)

# Set font size of y-axis tick labels
font_size_axis = 8
plt.xticks(fontsize=font_size_axis)
plt.yticks(fontsize=font_size_axis)

# Set font size of annotations
font_size_param = 8
plt.rcParams['font.size'] = font_size_param

# Set axis labels and title
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Random Confusion Matrix")

# Improve the layout
plt.tight_layout()

# Display the plot
plt.show()

import numpy as np

# Function for top5 label predictions
def predict_top5_labels(lt_value, tokenizer, model, label_encoder, max_length):
    # Tokenize and encode the lt_value using the BERT tokenizer
    encoding = tokenizer.encode_plus(
        lt_value,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='tf'  # Use TensorFlow tensors
    )

    # Extract the input_ids and attention_mask from the encoding
    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']

    # Make predictions using the model
    predictions = model.predict([input_ids, attention_mask])[0]
    top5_indices = np.argsort(predictions)[-5:][::-1]
    top5_labels = label_encoder.inverse_transform(top5_indices)
    top5_probs = predictions[top5_indices]

    return top5_labels, top5_probs

# Example usage
lt_input = 'GB106000:CP50C_T50C-12  CP50C Ex Aftercooler Margam     C'
# GB104020:WTurb1VibDxDE
##### was wird #####
# Apply lowercase
lt_input = lt_input.lower()
# Remove non-alphanumeric characters
lt_input = re.sub(r'[^a-zA-Z0-9\s]', ' ', lt_input)
# die ersten ein bis zwei buchstaben vom lt weg

# Extract the first one or two letters
extracted_letters = re.findall(r'^([A-Za-z]{1,2})', lt_input)

if extracted_letters:
    lt_input = lt_input + ' ' +  extracted_letters[0]
else:
    lt_input = ' '
##### was wird #####

predicted_labels, predicted_probs = predict_top5_labels(lt_input, tokenizer, model, label_encoder, max_length)
# SA000400:MAC2Stg1E1TempOut		ECM, Cooler Outlet Temperature Cooler 1, MAC2, Dammam	°C
print("Top 5 Predicted Labels:")
for label, prob in zip(predicted_labels, predicted_probs):
    print("Label:", label)
    print("Probability:", prob)
    print()

# Example usage
lt_input = 'SE451901:XIA-2232/AI1/PV.CV_H	RecC1 Vib Y HS1NDE high alarm DCS, Vastkust'
##### was wird #####
# Apply lowercase
lt_input = lt_input.lower()
# Remove non-alphanumeric characters
lt_input = re.sub(r'[^a-zA-Z0-9\s]', ' ', lt_input)
# die ersten ein bis zwei buchstaben vom lt weg

# Extract the first one or two letters
extracted_letters = re.findall(r'^([A-Za-z]{1,2})', lt_input)

if extracted_letters:
    lt_input = lt_input + ' ' +  extracted_letters[0]
else:
    lt_input = ' '
##### was wird #####
predicted_labels, predicted_probs = predict_top5_labels(lt_input, tokenizer, model, label_encoder, max_length)
# SA000400:MAC2Stg1E1TempOut		ECM, Cooler Outlet Temperature Cooler 1, MAC2, Dammam	°C
print("Top 5 Predicted Labels:")
for label, prob in zip(predicted_labels, predicted_probs):
    print("Label:", label)
    print("Probability:", prob)
    print()





# Save the trained model
model.save('/content/saved_model/model_directory/bert_model.h5')

# Save the tokenizer
tokenizer.save_pretrained('/content/saved_model/tokenizer_directory')
import joblib
# Save the label encoder
joblib.dump(label_encoder, '/content/saved_model/label_encoder.pkl')

import pickle

# Save the label encoder
with open('/content/saved_model/label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)





from google.colab import drive
drive.mount('/content/drive')

import shutil

# Specify the path of the model directory
model_directory_path = '/content/saved_model/model_directory'

# Specify the destination path for the zip file in Google Drive
drive_zip_path = '/content/drive/MyDrive/model.zip'

# Create the zip file
shutil.make_archive('/content/model', 'zip', model_directory_path)

# Copy the zip file to Google Drive
shutil.copy('/content/model.zip', drive_zip_path)

from transformers import TFBertModel
import tensorflow as tf

# Load the BERT model
model = TFBertModel.from_pretrained('bert-base-multilingual-cased')

# Load the trained weights of the model
model.load_weights('/content/saved_model/model_directory/bert_model.h5')

# Save the history
import joblib
joblib.dump(history, '/content/history_file.pkl')

# Load the history
history = joblib.load('/content/history_file.pkl')

import numpy as np
from sklearn.metrics import classification_report
import joblib


# Speichere die Testdaten
np.save('/content/vars/X_test.npy', X_test)
np.save('/content/vars/X_attention_test.npy', X_attention_test)

# Speichere die Vorhersagen
np.save('/content/vars/y_pred.npy', y_pred)

# Speichere die ursprünglichen Labels
np.save('/content/vars/y_test_labels.npy', y_test_labels)
np.save('/content/vars/y_test.npy', y_test)
np.save('/content/vars/y_pred_labels.npy', y_pred_labels)

# Speichere den Klassifikationsbericht
with open('/content/classification_report.txt', 'w') as file:
    file.write(report)

