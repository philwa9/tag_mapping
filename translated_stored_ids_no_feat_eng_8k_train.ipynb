import pandas as pd
import numpy as np
import re
!pip install transformers


file_path = '/content/mapped_7k_clean.xlsx'
sheet_name = '8k_train'
df = pd.read_excel(file_path, sheet_name=sheet_name)



from transformers.models.roberta_prelayernorm.modeling_roberta_prelayernorm import RobertaPreLayerNormEncoder
if 'gt öfter vorhanden?' in df.columns:
    df = df.drop('gt öfter vorhanden?', axis=1)


# strings
df['gt'] = df['gt'].astype(str)
df['lt'] = df['lt'].astype(str)

df['desc_gt'] = df['desc_gt'].astype(str)
df['desc_lt'] = df['desc_lt'].astype(str)

df['u_gt'] = df['u_gt'].astype(str)
df['u_lt'] = df['u_lt'].astype(str)


# pattern (side_id und alles nach komma weg)
pattern_sid = r'[A-Z]{2}\d{6}'

pattern_descgt = r',[^,]*$'

pattern_machine = r':[A-Za-z]{3,7}\d{1,2}'

pattern_ = r'_[A-Z]{0,3}'


#%%

machines_from_master = [
    'BAC',
    'CAB',
    'CBoost',
    'CGen',
    'CO2C',
    'COC',
    'CTurb',
    'CWP',
    'FeedC',
    'FGB',
    'GANC',
    'Gen',
    'GOXC',
    'H2C',
    'LPBoost',
    'LPGen',
    'LPTurb',
    'MAC',
    'NH3C',
    'PAIRC',
    'RecC',
    'Ref',
    'RefC',
    'SynC',
    'VP',
    'WBoost',
    'WGen',
    'WTurb',
    'LiqTurb',
    'SteamTurb',
    'GasTurb',
    'CTF',
    'LOXP',
    'LINP',
    'LARP',
    'EJF',
    'CCM',
    'IPC',
    'CWS',
    'DCAC',
    'DCWC',
    'FeedPT',
    'GSFR',
    'MHE',
    'PPU',
    'PreRef',
    'PSA',
    'Reform',
    'RHE',
    'Shift',
    'SWGR',
    'Tank',
    'Tank',
    'TRF',
    'Stripper',
    'Liquifier',
    'PreCl',
    'Chlr',
    'Cond',
    'Rcv',
    'Econ',
    'SubCl',
    'Dryer',
    'Catox',
    'LPCol',
    'MPCol',
    'HPCol',
    'CArCol',
    'PArCol',
    'PAirC'
    'KrXeCol',
    'SpargeTank',
    'LOXTank',
    'LOXVAP',
    'LINTANK',
    'LARTank',
    'LINVAP',
    'RECC',
    'LOXTANK',
    'GOX',
    'ASC',
    'GAN',
]


#%%



# List to store replaced items
machine_tag = []


for index, entry in enumerate(df['gt']):
    match = None
    for machine in machines_from_master:
        pattern = rf'\b{machine}\d'
        if re.search(pattern, entry):
            match = re.findall(pattern, entry)
            break
    if match:
        machine_tag.append(match[0])
        df.at[index, 'machine_tag'] = match[0]
        df.at[index, 'gt'] = df.at[index, 'gt'].replace(match[0], '')



#%%
import re


df['side_id'] = None  # Adding a new column with None values

for index, entry in enumerate(df['gt']):
    # Find the pattern using re.search
    match = re.search(pattern_sid, str(entry))

    if match:
        # Store found pattern
        df.at[index, 'side_id'] = match.group(0)

        # Remove the found pattern from the 'gt' column
        df.at[index, 'gt'] = str(entry).replace(pattern_sid, '')


import pickle


# file_path = '/Users/philippwarter/Desktop/MAPR - Forschungsmaster/Projektarbeit 1. Semester/Implementierung des nametag-matching Algorithmus/DeepL API/translation_cache_30_06.pkl'

file_path_transcache = '/content/translation_cache_30_06.pkl'

# Laden des translation_cache 
with open(file_path_transcache, 'rb') as file:
    loaded_translation_cache = pickle.load(file)

print("translation_cache wurde erfolgreich aus", file_path_transcache, "geladen.")


df['desc_lt'] = df['desc_lt'].fillna('')
df['desc_lt'] = df['desc_lt'].astype(str)

#%%

# Ersetzen von Umlauten und diakritischen Zeichen
diacritics = {
    'ü': 'ue', 'Ü': 'UE', 'ä': 'ae', 'Ä': 'AE', 'ö': 'oe', 'Ö': 'OE',
    'é': 'e', 'É': 'E', 'á': 'a', 'Á': 'A', 'à': 'a', 'À': 'A', 'è': 'e', 'È': 'E', 'ß': 'ss'
    # Weitere diakritische Zeichen hier hinzufügen
}
df['desc_lt'] = df['desc_lt'].replace(diacritics, regex=True)



df['desc_lt'] = df['desc_lt'].str.lower()


# zahlen zum Ersetzen
number_dict = {'1st': '1', '1rst': '1', '2nd': '2', '3rd': '3', '4th': '4', '5th': '5', '6th': '6', '7th': '7', '8th': '8', '9th': '9', '10th': '10', '1ST': '1', '2ND': '2', '3RD': '3', '4TH': '4', '5TH': '5', '6TH': '6', '7TH': '7', '8TH': '8', '9TH': '9', '10TH': '10'}

# Ersetzen von zahlen
df['desc_lt'] = df['desc_lt'].replace(number_dict, regex=True)


# Neue Spalte zum Speichern der entfernten Teile erstellen
df['removed_parts'] = ''

translate_pat = r'[a-z]{4,25}'

df['removed_parts'] = df['desc_lt'].apply(lambda x: re.findall(translate_pat, x))




#%%

# Entfernen von nicht-alphanumerischen Zeichen
df['desc_lt'] = df['desc_lt'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', ' ', x))

df['desc_lt'] = df['desc_lt'].str.lstrip()

df['removed_parts'] = df['removed_parts'].apply(lambda x: ' '.join(x))
#%%

!pip install deepl
import re
import deepl

target_language = 'EN-GB'
auth_key = '41a3391e-e6eb-e404-c4ef-b30ac03bbb05:fx'
translate_pat = r'[a-z]{4,25}'



#%%
def translate_word(match):
    word = match.group(0)

    if word in loaded_translation_cache:
        translated_word = loaded_translation_cache[word]
    else:
        translator = deepl.Translator(auth_key)
        result = translator.translate_text(word, target_lang=target_language)
        translated_word = result.text
        loaded_translation_cache[word] = translated_word

    return translated_word

df['modified_row'] = ''
df['translated_words'] = ''

for i in range(len(df)):
    desc_lt = df.at[i, 'desc_lt']
    gt = df.at[i, 'gt']

    if re.search('(DE|FR)', gt):
        modified_row = re.sub(translate_pat, translate_word, desc_lt)
        df.at[i, 'modified_row'] = modified_row

        translated_words = re.findall(translate_pat, desc_lt)
        translated_words = [loaded_translation_cache.get(word, word) for word in translated_words]
        translated_words = ' '.join(translated_words)
        df.at[i, 'translated_words'] = translated_words
    else:
        df.at[i, 'modified_row'] = desc_lt
        df.at[i, 'translated_words'] = ''


for i in range(len(df)):
    df.at[i, 'desc_lt'] = df.at[i, 'modified_row']




#%%
# Create df from machine_tag list
df_machine_tag = pd.DataFrame(machine_tag, columns=['Machine Tag'])

# Replace ":" # der hier löscht wieder nur nach dem standard pattern ohne machine_master
df_machine_tag['Machine Tag'] = df_machine_tag['Machine Tag'].str.replace(':', '')

df_machine_tag['Machine Tag'] = df_machine_tag['Machine Tag'].dropna()
df_machine_tag.dropna(subset=['Machine Tag'], inplace=True)


# aus 6 spalten 2 machen

df['gt'] = df.apply(lambda row: ' '.join([str(row['gt'])]), axis=1)

df['lt'] = df.apply(lambda row: ' '.join([str(row['lt']), str(row['desc_lt']), str(row['u_lt'])]), axis=1)


# restliche spalten löschen

df = df.drop(['desc_gt', 'u_gt', 'desc_lt', 'u_lt', 'removed_parts', 'modified_row', 'translated_words'], axis=1)

#%%
df['gt'] = df['gt'].str.replace(pattern_sid, '')

df['gt'] = df['gt'].str.replace(':', '')

df['lt'] = df['lt'].str.replace(pattern_sid, '')

df['lt'] = df['lt'].str.replace(':', '')

# notgedrungen funktion die die erste digit löscht:

            # unbedingt fixen!!

def remove_first_digit(entry):
    if entry[0].isdigit():
        return entry[1:]
    return entry

df['gt'] = df['gt'].apply(remove_first_digit)

unique_entries = df['gt'].unique()

excel_trans_df = "/content/translated_df.xlsx"
df.to_excel(excel_trans_df, index=False)

print(f"Test DF wurde in {excel_trans_df} gespeichert.")


import tensorflow as tf
# from transformers import TFBertModel, BertTokenizer
from transformers import RobertaTokenizer, TFRobertaModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


# tokenize and encode 
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')


import numpy as np

max_length = 50  # Set your desired max sequence length here

X_encoded = []
for text in df['lt']:
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='np'
    )
    X_encoded.append(encoding)

# convert list of encoded inputs to numpy arrays
X_input = np.concatenate([encoding['input_ids'] for encoding in X_encoded])
X_attention = np.concatenate([encoding['attention_mask'] for encoding in X_encoded])

# label encode gt
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['gt'])

# split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_input, y, test_size=0.1, random_state=42)

# split the attention masks into training and testing sets 
X_attention_train, X_attention_test, _, _ = train_test_split(X_attention, y, test_size=0.1, random_state=42)

# build model with BERT embeddings
input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32)
attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32)

bert_model = TFRobertaModel.from_pretrained('roberta-base')
bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]
output = tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')(bert_output[:, 0, :])

model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)



# compile and train the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit([X_train, X_attention_train], y_train,
                    epochs=43, batch_size=128,
                    validation_data=([X_test, X_attention_test], y_test))



import matplotlib.pyplot as plt


train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']


plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(train_loss, label='Train Loss')
plt.plot(val_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Train and Validation Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Train and Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()



from sklearn.metrics import classification_report

print(X_test.size)
print(X_attention_test.size)

#%%

# Perform predictions on the test set
y_pred = np.argmax(model.predict([X_test, X_attention_test]), axis=-1)

#%%

# Inverse transform the label encodings
y_test_labels = label_encoder.inverse_transform(y_test)
y_pred_labels = label_encoder.inverse_transform(y_pred)

#%%

# Calculate precision, recall, and F1 score
report = classification_report(y_test_labels, y_pred_labels)
print("Classification Report:\n", report)


import matplotlib.pyplot as plt
import seaborn as sns
import random
from sklearn.metrics import confusion_matrix


#### Random Samples aus Daten für die CM ######
# Generate confusion matrix
cm = confusion_matrix(y_test_labels, y_pred_labels)
print("Confusion Matrix:\n", cm)

# Retrieve original labels from label encoder
original_labels = label_encoder.inverse_transform(range(len(label_encoder.classes_)))

# Randomly select  entries
random_indices = random.sample(range(len(cm)), 25)
random_cm = cm[random_indices, :][:, random_indices]
random_labels = original_labels[random_indices]

# Create a figure and axis
plt.figure(figsize=(10, 10), dpi=150)  # Adjust figsize and dpi as needed

sns.heatmap(random_cm, annot=True, fmt="d", cmap="Blues", cbar=False, xticklabels=random_labels, yticklabels=random_labels)

# Set font size of y-axis tick labels
font_size_axis = 8
plt.xticks(fontsize=font_size_axis)
plt.yticks(fontsize=font_size_axis)

# Set font size of annotations
font_size_param = 8
plt.rcParams['font.size'] = font_size_param

# Set axis labels and title
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Random Confusion Matrix")

# Improve the layout
plt.tight_layout()

# Display the plot
plt.show()



# Load the data
file_path = '/content/mapped_7k_clean.xlsx'
sheet_name = '8k_test'
test_df = pd.read_excel(file_path, sheet_name=sheet_name)

if 'gt öfter vorhanden?' in test_df.columns:
    test_df = test_df.drop('gt öfter vorhanden?', axis=1)



# strings
test_df['gt'] = test_df['gt'].astype(str)
test_df['lt'] = test_df['lt'].astype(str)

test_df['desc_gt'] = test_df['desc_gt'].astype(str)
test_df['desc_lt'] = test_df['desc_lt'].astype(str)

test_df['u_gt'] = test_df['u_gt'].astype(str)
test_df['u_lt'] = test_df['u_lt'].astype(str)


# pattern (side_id und alles nach komma weg)
pattern_sid = r'[A-Z]{2}\d{6}'

pattern_descgt = r',[^,]*$'

pattern_machine = r':[A-Za-z]{3,7}\d{1,2}'

pattern_ = r'_[A-Z]{0,3}'


#%%

machines_from_master = [
    'BAC',
    'CAB',
    'CBoost',
    'CGen',
    'CO2C',
    'COC',
    'CTurb',
    'CWP',
    'FeedC',
    'FGB',
    'GANC',
    'Gen',
    'GOXC',
    'H2C',
    'LPBoost',
    'LPGen',
    'LPTurb',
    'MAC',
    'NH3C',
    'PAIRC',
    'RecC',
    'Ref',
    'RefC',
    'SynC',
    'VP',
    'WBoost',
    'WGen',
    'WTurb',
    'LiqTurb',
    'SteamTurb',
    'GasTurb',
    'CTF',
    'LOXP',
    'LINP',
    'LARP',
    'EJF',
    'CCM',
    'IPC',
    'CWS',
    'DCAC',
    'DCWC',
    'FeedPT',
    'GSFR',
    'MHE',
    'PPU',
    'PreRef',
    'PSA',
    'Reform',
    'RHE',
    'Shift',
    'SWGR',
    'Tank',
    'Tank',
    'TRF',
    'Stripper',
    'Liquifier',
    'PreCl',
    'Chlr',
    'Cond',
    'Rcv',
    'Econ',
    'SubCl',
    'Dryer',
    'Catox',
    'LPCol',
    'MPCol',
    'HPCol',
    'CArCol',
    'PArCol',
    'PAirC'
    'KrXeCol',
    'SpargeTank',
    'LOXTank',
    'LOXVAP',
    'LINTANK',
    'LARTank',
    'LINVAP',
    'RECC',
    'LOXTANK',
    'GOX',
    'ASC',
    'GAN',
]


#%%



# List to store replaced items
machine_tag = []


for index, entry in enumerate(test_df['gt']):
    match = None
    for machine in machines_from_master:
        pattern = rf'\b{machine}\d'
        if re.search(pattern, entry):
            match = re.findall(pattern, entry)
            break
    if match:
        machine_tag.append(match[0])
        test_df.at[index, 'machine_tag'] = match[0]
        test_df.at[index, 'gt'] = test_df.at[index, 'gt'].replace(match[0], '')



#%%
import re

# Assuming match and pattern_sid are defined earlier in your code
test_df['side_id'] = None  # Adding a new column with None values

for index, entry in enumerate(test_df['gt']):
    # Find the pattern using re.search
    match = re.search(pattern_sid, str(entry))

    if match:
        # Store the found pattern in the 'side_id' column
        test_df.at[index, 'side_id'] = match.group(0)

        # Remove the found pattern from the 'gt' column
        test_df.at[index, 'gt'] = str(entry).replace(pattern_sid, '')


import pickle

# Pfad zur geladenen Datei
file_path_transcache = '/content/translation_cache_30_06.pkl'

# Laden des translation_cache aus der Datei
with open(file_path_transcache, 'rb') as file:
    loaded_translation_cache = pickle.load(file)

print("translation_cache wurde erfolgreich aus", file_path_transcache, "geladen.")


test_df['desc_lt'] = test_df['desc_lt'].fillna('')
test_df['desc_lt'] = test_df['desc_lt'].astype(str)

#%%

# Ersetzen von Umlauten und diakritischen Zeichen
diacritics = {
    'ü': 'ue', 'Ü': 'UE', 'ä': 'ae', 'Ä': 'AE', 'ö': 'oe', 'Ö': 'OE',
    'é': 'e', 'É': 'E', 'á': 'a', 'Á': 'A', 'à': 'a', 'À': 'A', 'è': 'e', 'È': 'E', 'ß': 'ss'
    # Weitere diakritische Zeichen hier hinzufügen
}
test_df['desc_lt'] = test_df['desc_lt'].replace(diacritics, regex=True)



test_df['desc_lt'] = test_df['desc_lt'].str.lower()


# zahlen zum Ersetzen
number_dict = {'1st': '1', '1rst': '1', '2nd': '2', '3rd': '3', '4th': '4', '5th': '5', '6th': '6', '7th': '7', '8th': '8', '9th': '9', '10th': '10', '1ST': '1', '2ND': '2', '3RD': '3', '4TH': '4', '5TH': '5', '6TH': '6', '7TH': '7', '8TH': '8', '9TH': '9', '10TH': '10'}

# Ersetzen von zahlen
test_df['desc_lt'] = test_df['desc_lt'].replace(number_dict, regex=True)


# Neue Spalte zum Speichern der entfernten Teile erstellen
test_df['removed_parts'] = ''

translate_pat = r'[a-z]{4,25}'

test_df['removed_parts'] = test_df['desc_lt'].apply(lambda x: re.findall(translate_pat, x))




#%%

# Entfernen von nicht-alphanumerischen Zeichen
test_df['desc_lt'] = test_df['desc_lt'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', ' ', x))

test_df['desc_lt'] = test_df['desc_lt'].str.lstrip()

test_df['removed_parts'] = test_df['removed_parts'].apply(lambda x: ' '.join(x))
#%%


import re
import deepl

target_language = 'EN-GB'
auth_key = '41a3391e-e6eb-e404-c4ef-b30ac03bbb05:fx'
translate_pat = r'[a-z]{4,25}'



#%%
def translate_word(match):
    word = match.group(0)

    if word in loaded_translation_cache:
        translated_word = loaded_translation_cache[word]
    else:
        translator = deepl.Translator(auth_key)
        result = translator.translate_text(word, target_lang=target_language)
        translated_word = result.text
        loaded_translation_cache[word] = translated_word

    return translated_word

test_df['modified_row'] = ''
test_df['translated_words'] = ''

for i in range(len(test_df)):
    desc_lt = test_df.at[i, 'desc_lt']
    gt = test_df.at[i, 'gt']

    if re.search('(DE|FR)', gt):
        modified_row = re.sub(translate_pat, translate_word, desc_lt)
        test_df.at[i, 'modified_row'] = modified_row

        translated_words = re.findall(translate_pat, desc_lt)
        translated_words = [loaded_translation_cache.get(word, word) for word in translated_words]
        translated_words = ' '.join(translated_words)
        test_df.at[i, 'translated_words'] = translated_words
    else:
        test_df.at[i, 'modified_row'] = desc_lt
        test_df.at[i, 'translated_words'] = ''


for i in range(len(test_df)):
    test_df.at[i, 'desc_lt'] = test_df.at[i, 'modified_row']




#%%
# Create a DataFrame from the machine_tag list
test_df_machine_tag = pd.DataFrame(machine_tag, columns=['Machine Tag'])

# Replace the ':' character in the 'Machine Tag' column # der hier löscht wieder nur nach dem standard pattern ohne machine_master
test_df_machine_tag['Machine Tag'] = test_df_machine_tag['Machine Tag'].str.replace(':', '')

test_df_machine_tag['Machine Tag'] = test_df_machine_tag['Machine Tag'].dropna()
test_df_machine_tag.dropna(subset=['Machine Tag'], inplace=True)


# aus 6 spalten 2 machen

test_df['gt'] = test_df.apply(lambda row: ' '.join([str(row['gt'])]), axis=1)

test_df['lt'] = test_df.apply(lambda row: ' '.join([str(row['lt']), str(row['desc_lt']), str(row['u_lt'])]), axis=1)


# restliche spalten löschen

test_df = test_df.drop(['desc_gt', 'u_gt', 'desc_lt', 'u_lt', 'removed_parts', 'modified_row', 'translated_words'], axis=1)

#%%
test_df['gt'] = test_df['gt'].str.replace(pattern_sid, '')

test_df['gt'] = test_df['gt'].str.replace(':', '')

test_df['lt'] = test_df['lt'].str.replace(pattern_sid, '')

test_df['lt'] = test_df['lt'].str.replace(':', '')

# notgedrungen scheiß funktion die die erste digit löscht:

            # unbedingt fixen!!

def remove_first_digit(entry):
    if entry[0].isdigit():
        return entry[1:]
    return entry

test_df['gt'] = test_df['gt'].apply(remove_first_digit)

unique_entries = test_df['gt'].unique()

excel_trans_test_df = "/content/translated_test_df.xlsx"
test_df.to_excel(excel_trans_test_df, index=False)

# Gib eine Bestätigung aus
print(f"Der Testdatenrahmen wurde in die Excel-Datei {excel_trans_test_df} gespeichert.")



def predict_with_confidence(model, tokenizer, lt_input):
    # Tokenize and encode the new input using BERT tokenizer
    encoding = tokenizer.encode_plus(
        lt_input,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='np'
    )

    # Get the input IDs and attention mask
    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']

    # Make the prediction using the trained model
    prediction = model.predict([input_ids, attention_mask])

    # Get the predicted class index
    predicted_class_index = np.argmax(prediction)

    # Get the confidence/probability of the predicted class
    confidence = prediction[0, predicted_class_index]

    # Decode the predicted class using the label encoder
    predicted_class = label_encoder.classes_[predicted_class_index]

    return predicted_class, confidence


# Annahme: Du hast einen Testdatenrahmen namens df_test mit den Spalten 'gt' und 'lt' (echtes Label und Input)
# Erstelle leere Spalten für die Vorhersage und die Konfidenz
test_df['prediction'] = ''
test_df['confidence'] = 0.0  # Initialisiere die Konfidenz mit 0.0

# Iteriere über die Zeilen des Testdatenrahmens
for index, row in test_df.iterrows():
    lt_input = row['lt']  # Nehme den Input aus der 'lt'-Spalte

    # Mache die Vorhersage mit der Funktion
    predicted_class, confidence = predict_with_confidence(model, tokenizer, lt_input)

    # Schreibe die Vorhersage und die Konfidenz in den Datenrahmen
    test_df.at[index, 'prediction'] = predicted_class
    test_df.at[index, 'confidence'] = confidence

# Annahme: df_test wurde bereits erstellt und aktualisiert

# Speichere den Testdatenrahmen in eine Excel-Datei
excel_filename = "/content/evaluation_test.xlsx"
test_df.to_excel(excel_filename, index=False)

# Gib eine Bestätigung aus
print(f"Der Testdatenrahmen wurde in die Excel-Datei {excel_filename} gespeichert.")




!pip install fuzzywuzzy


from fuzzywuzzy import fuzz

# Funktion zur Berechnung der Fuzzy-Ähnlichkeit
def calculate_fuzzy_score(row):
    gt = row['gt']
    pred_gt = row['prediction']

    # Verwende die Funktion "fuzz.token_sort_ratio" für die Fuzzy-Ähnlichkeitsberechnung
    score = fuzz.token_sort_ratio(gt, pred_gt)

    return score

# Füge die "fuzzy_score"-Spalte dem DataFrame hinzu
test_df['fuzzy_score'] = test_df.apply(calculate_fuzzy_score, axis=1)

# Speichere den Testdatenrahmen in eine Excel-Datei
excel_filename = "/content/evaluation_test.xlsx"
test_df.to_excel(excel_filename, index=False)

# Gib eine Bestätigung aus
print(f"Der Testdatenrahmen wurde in die Excel-Datei {excel_filename} gespeichert.")
